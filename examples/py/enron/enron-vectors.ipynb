{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6785c319",
   "metadata": {},
   "source": [
    "# Using Raphtory similarity search to uncover Enron criminal network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd195489",
   "metadata": {},
   "source": [
    "The Enron scandal was one of the largest corporate fraud cases in history, leading to the downfall of the company and the conviction of several executives. The graph below illustrates the significant decline in Enron's stock price between August 2000 and December 2001, providing valuable insights into the company's downfall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce5767",
   "metadata": {},
   "source": [
    "![enron stock price](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/EnronStockPriceAugust2000toJanuary2001.svg/567px-EnronStockPriceAugust2000toJanuary2001.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5521bb",
   "metadata": {},
   "source": [
    "Now, put yourself in the judge's seat, confronted with a vast dataset comprising hundreds of thousands of emails. Your responsibility? Identifying every culprit and related elements within. How long would it take to uncover all of them and their connections?\n",
    "\n",
    "Prepare for a revelation: Raphtory now boasts seamless similarity search functionality. This enables swift exploration across the entire network of email messages, swiftly pinpointing diverse criminal activities. All it takes is submitting a semantic query pertaining to the specific crimes under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7667c42",
   "metadata": {},
   "source": [
    "## Wait, how does this work? And what is similarity search in the first place?\n",
    "\n",
    "Similarity search isn't a novel concept. It's a powerful technique that sifts through a collection of documents to identify those bearing semantic resemblance to a given query.\n",
    "\n",
    "Consider a query like `hiding information`. Imagine applying this query across a corpus of email messages; the result would likely yield documents discussing various aspects of concealing information.\n",
    "\n",
    "And which role does that play in Raphtory land? To traverse the bridge, we must represent entities within graphs as documents or sets of documents. These documents are then transformed into embeddings or vectors using an embedding function for effective searchability. In Raphtory, this process is referred to as 'vectorising' a graph, and it is as easy as:\n",
    "\n",
    "```\n",
    "vg = g.vectorise(embeddding_function)\n",
    "```\n",
    "\n",
    "Raphtory has a default way to translate graph entities into documents. However, if we have a deep understanding of our graph's semantics, we can always create those documents ourselves, insert them as properties, and let Raphtory know which property name use to pick them up:\n",
    "\n",
    "```\n",
    "g.add_node(0, 'Kenneth Lay', {'document': 'Kenneth Lay is the former CEO of Enron'})\n",
    "vg = g.vectorise(embeddding_function, nodes=\"document\")\n",
    "```\n",
    "\n",
    "Voila! Executing a similarity search query on the graph is now straightforward. Using methods within `VectorisedGraph`, we can select and retrieve documents based on a query:\n",
    "\n",
    "```\n",
    "vg.append_by_similarity('hiding information', limit=10).get_documents()\n",
    "```\n",
    "\n",
    "This example is a basic query, capturing the top 10 highest-scoring documents. However, Raphtory offers an array of advanced methods, enabling the implementation of complex similarity search algorithms. You can combine different queries into a single selection or even leverage the graph's space between documents to add more context to one selection using an similarity based expansion.\n",
    "\n",
    "Now, armed with these fundamentals, let's embark on the quest to unearth some potential criminals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcf479",
   "metadata": {},
   "source": [
    "## Preparing the investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ceb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from raphtory import *\n",
    "from raphtory import algorithms\n",
    "from raphtory import export\n",
    "from raphtory.vectors import *\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from email.utils import parsedate_to_datetime, parsedate\n",
    "from datetime import timezone, datetime\n",
    "from time import mktime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41dc7f",
   "metadata": {},
   "source": [
    "First, we define some auxiliary functions for parsing of the Enron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97aba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sender(text):\n",
    "    sender_cut = text.split(\"\\nFrom: \")\n",
    "    if len(sender_cut) > 1:\n",
    "        email_cut =  sender_cut[1].split(\"\\n\")[0].split(\"@\")\n",
    "        if len(email_cut) > 1:\n",
    "            return email_cut[0]\n",
    "        else: \n",
    "            return\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def extract_sender_domain(text):\n",
    "    sender_cut = text.split(\"\\nFrom: \")\n",
    "    if len(sender_cut) > 1:\n",
    "        email_cut =  sender_cut[1].split(\"\\n\")[0].split(\"@\")\n",
    "        if len(email_cut) > 1:\n",
    "            return email_cut[1]\n",
    "        else: \n",
    "            return\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def extract_recipient(text):\n",
    "    recipient_cut = text.split(\"\\nTo: \")\n",
    "    if len(recipient_cut) > 1:\n",
    "        email_cut = recipient_cut[1].split(\"\\n\")[0].split(\"@\")\n",
    "        if len(email_cut) > 1:\n",
    "            return email_cut[0]\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def extract_actual_message(text):\n",
    "    try:\n",
    "        body = re.split(\"X-FileName: .*\\n\\n\", text)[1]\n",
    "        return re.split('-{3,}\\s*Original Message\\s*-{3,}', body)[0][:1000]\n",
    "    except:\n",
    "        return\n",
    "\n",
    "extract_date = lambda text: text.split(\"Date: \")[1].split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e943817",
   "metadata": {},
   "source": [
    "Then, we ingest the email dataset and carry out some cleaning using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de7a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron = pd.DataFrame()\n",
    "enron['email'] = pd.read_csv('emails.csv', usecols=['message'])['message']\n",
    "enron['src'] = enron['email'].apply(extract_sender)\n",
    "enron['dst'] = enron['email'].apply(extract_recipient)\n",
    "enron['time'] = enron['email'].apply(extract_date)\n",
    "enron['message'] = enron['email'].apply(extract_actual_message)\n",
    "enron['message'] = enron['message'].str.strip()\n",
    "\n",
    "enron = enron.dropna(subset=[\"src\", \"dst\", \"time\", \"message\"])\n",
    "enron = enron.drop_duplicates(['src', 'dst', 'time', 'message'])\n",
    "enron = enron[enron['message'].str.len() > 5]\n",
    "enron = enron[enron['dst'] != 'undisclosed.recipients']\n",
    "enron = enron[enron['email'].apply(extract_sender_domain) == 'enron.com']\n",
    "\n",
    "enron['document'] = enron['src'] + \" sent a message to \" + enron['dst'] + \" at \" + enron['time'] + \" with the following content:\\n\" + enron['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15433650",
   "metadata": {},
   "source": [
    "Next, we ingest those emails into a Raphtory graph. Here individuals serve as nodes,\n",
    "most of them belonging to Enron, and the edges repesent email exchanges between them. Our criminal investigation targets the last four months of 2001, coinciding with the Enron bankruptcy, so we will create a window over the graph for that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6496d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_graph = Graph()\n",
    "def ingest_edge(record):\n",
    "    e = raw_graph.add_edge(record['time'], record['src'], record['dst'], {'document': record['document']})\n",
    "    raw_graph.add_node(record['time'], record['src']).add_constant_properties({'document': ''})\n",
    "    raw_graph.add_node(record['time'], record['dst']).add_constant_properties({'document': ''})\n",
    "enron.apply(ingest_edge, axis=1)\n",
    "g = raw_graph.window('2001-09-01 00:00:00', '2002-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b7549",
   "metadata": {},
   "source": [
    "And our `vectors` module comes into play at this stage. We are going to vectorise the graph we just built. As previously outlined, this involves employing an embedding function that translates documents into vectors. For this purpose, we've selected a local model from Langchain named `gte-small`. It's important to note that this operation is computationally very expensive. When initiated from scratch, the process can span several hours. However, to streamline this, the vectorising process enables the setup of a cache file. By utilizing this cache, embeddings for previously processed documents are readily available, avoiding the need to invoke the resource-intensive model repeatedly. Fortunately, we've already taken this step for you, and there already exists a file named `embedding-cache` in the current directory, containing all the necessary embeddings for today's task, so the execution will be instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79cd8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f514d2a6944e2784c9e7cfa5e5e4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b78dd76e0647168612803961dd6357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f5d1679ca247db85abce518c9b053c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69db5e41483340f48aabb36c2e821888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b4cb6fb1a147bab93a798f2c608c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd05358aa594123bd84127a418816cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/66.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b648d6b245545a3a9472cdcc8657489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa303e1efec409fb46e7f195b66d66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4e0fceded64416bfc4a447c054f205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49504064ca0b48b294bf51b0a69cd5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff182c9225349c1af362802fd3ca469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be56632a88645f79a46b6121357890d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing embeddings for nodes\n",
      "computing embeddings for edges\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "embedding_function = lambda texts: embeddings.embed_documents(texts)\n",
    "\n",
    "vg = g.vectorise(\n",
    "    embedding_function,\n",
    "    \"./embedding-cache\",\n",
    "    node_document=\"document\",\n",
    "    edge_document=\"document\",\n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecacfe0",
   "metadata": {},
   "source": [
    "## Finding the criminal network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a6e34",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully loaded a vectorised graph containing four critical months of the Enron email dataset. Using our similarity search engine, we can now submit queries to uncover potential criminal behavior or, at the very least, steer the investigation in the right direction. Our aim is to identify individuals who might hold pertinent information regarding Enron's internal practices. You're encouraged to experiment with your own queries and methodologies. However, for starters, we'll concentrate on three pivotal topics that were crucial in past investigations:\n",
    "1. Hiding company debt through special purpose entities (SPEs)\n",
    "2. Manipulation of the energy market\n",
    "3. Withholding crucial information from investors\n",
    "\n",
    "We will make use of the following auxiliary functions to help us get the job done. Feel free to explore these functions for insights on handling common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228a51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_emails(query, limit):\n",
    "    for doc in vg.append_edges_by_similarity(query, limit).get_documents():\n",
    "        print(doc.content)\n",
    "        print('===========================================================================================')\n",
    "        \n",
    "def show_network_for_query(query, limit):\n",
    "    edges = vg.append_edges_by_similarity(query, limit).edges()\n",
    "    network = Graph()\n",
    "    for edge in edges:\n",
    "        network.add_edge(0, edge.src.name, edge.dst.name)\n",
    "\n",
    "    network = export.to_pyvis(graph=network, edge_color=\"#FF0000\")\n",
    "    return network.show('nx.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27856c61",
   "metadata": {},
   "source": [
    "### Hiding company debt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361f6b9",
   "metadata": {},
   "source": [
    "To uncover pertinent communications on this subject, we will use the query:\n",
    "- `hide company debt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c74b3c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rod.hayslett sent a message to james.saunders at Sun, 18 Nov 2001 08:05:22 -0800 (PST) with the following content:\n",
      "That is one of the things we are working on.   At this point in time Dynegy will be responsible for this debt, if theyexercise their rights under the preferred stock agreements, which would leave them with common pledged to the lenders.  The price they paid recognized the debt was there, if it is not there, the price will be higher.   Suffice it to say all of these things will be tken care of before it funds.\n",
      "--------------------------\n",
      "Sent from my BlackBerry Wireless Handheld (www.BlackBerry.net)\n",
      "===========================================================================================\n",
      "mariella.mahan sent a message to stanley.horton at Tue, 13 Nov 2001 12:53:54 -0800 (PST) with the following content:\n",
      "Something for us to talk about during our next staff meeting.\n",
      "\n",
      "There are three projects which have significant cash flow problems and thus=\n",
      " difficulties in meeting debt obligations: these are: SECLP, Panama and Gaz=\n",
      "a.  In the past, as I suppose we have done in Dabhol, we have taken the pos=\n",
      "ition that we would not inject cash into these companies and would be prepa=\n",
      "red to face a default and possible acceleration of the loans.  SECLP has be=\n",
      "en the biggest issue/problem.  Panama is much less (a few million of floati=\n",
      "ng of our receivables from the company) would be sufficient to meet the cas=\n",
      "h crunch in April of this year.  Note that, in Panama, the debt is fully gu=\n",
      "aranteed by the government and is non-recoursed to the operating company, B=\n",
      "LM.  In the past, we have discussed letting the debt default, which would c=\n",
      "ause the bank to potentially seek complete payment and acceleration from th=\n",
      "e GoPanama.  The reason: the vast majority of BLM's problems stem from acti=\n",
      "ons taken by\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_emails(\"hide company debt\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc644e0",
   "metadata": {},
   "source": [
    "Here, we can find an interesting email in the second position. This message highlights significant cash flow problems in three projects (SECLP, Panama, and Gaza) that face difficulties in meeting debt obligations. It discusses the position of not injecting cash into these companies and being prepared to face default and possible loan acceleration. This might be a good starting point for some investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716ab6f",
   "metadata": {},
   "source": [
    "### Manipulation of the energy market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e22285",
   "metadata": {},
   "source": [
    "Among the charges leveled against Enron were allegations of manipulating the energy market leveraging their influential position. To explore conversations pertaining to this matter and potentially uncover concrete evidence, we'll employ the query:\n",
    "- `manipulating the energy market`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97244400",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paulo.issler sent a message to zimin.lu at Mon, 22 Oct 2001 09:49:02 -0700 (PDT) with the following content:\n",
      "Ed has written the draft for the first assignment. I will be checking it today and making my comments by tomorrow morning. Feel free to make yours. I beleive the book Mananging Energy Price Risk is a great reference for the second assignment.     \n",
      "\n",
      "Thanks.\n",
      "Paulo Issler\n",
      "===========================================================================================\n",
      "bill.williams sent a message to alan.comnes at Thu, 25 Oct 2001 12:59:06 -0700 (PDT) with the following content:\n",
      "Alan,\n",
      "\n",
      "I have a few questions regarding the emminent implementation of the new target pricing mechanism.\n",
      "1. Does uninstructed energy still get paid (if not, we cannot hedge financials)\n",
      "2. The CISO Table 1. lists an unintended consequence as \" Target price may be manipulated due to no obligation to deliver\"\n",
      "\tWhy is there no obligation to deliver?\n",
      "3. Is there still a load deviation penalty? Or would that be considered seperately?\n",
      "\n",
      "Thanks for the help.\n",
      "\n",
      "Bill\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_emails(\"manipulating the energy market\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7757c",
   "metadata": {},
   "source": [
    "Here, in the second returned email, discussions revolve around legal boundaries concerning the new target pricing mechanism. They highlight the potential for manipulation within the new mechanism due to the absence of an obligation to deliver. While this doesn't explicitly confirm illegal conduct, individuals engaged in this conversation might possess valuable insights to elucidate Enron's practices concerning this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c5598",
   "metadata": {},
   "source": [
    "### Withholding crucial information from investors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03a72c",
   "metadata": {},
   "source": [
    "Finally, to address the last point of our investigation, we'll employ the query:\n",
    "\n",
    "- `lie to investors`\n",
    "\n",
    "This time we'll just show the subgraph comprising these communications, aiming to uncover the network of individuals that might be involved in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17aa6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "nx.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"nx.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x13852b1d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_network_for_query(\"lie to investors\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a54a1b",
   "metadata": {},
   "source": [
    "And at the very center of this network, we find Kenneth Lay, the former Enron CEO. He was indeed pleaded not guilty to eleven criminal charges. He was convicted of six counts of securities and wire fraud and was subject to a maximum of 45 years in prison. However, Lay died on July 5, 2006, before sentencing was to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc59bc",
   "metadata": {},
   "source": [
    "## Bonus: Integrating Raphtory with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5eb00f",
   "metadata": {},
   "source": [
    "As a bonus for this tutorial, we are going to look at how we can easily integrate Raphtory with a Large Language Model (LLM). There are many ways we can accomplish this, but leveraging the Langchain ecosystem seems like an excellent starting point. One of the options is defining a langchain Retriever using Raphtory. This allows the creation of various chains for diverse purposes such as Question/Answer setups, or agent-driven pipelines.\n",
    "\n",
    "In this example, we'll build the most basic QA setup, a Retrieval-augmented generation (RAG) pipeline. This kind of pipeline icombines of a document retriever and an LLM. When a question is submitted to the pipeline, that question initially goes through the document retriever, which extract relevants documents from a set. These documents are then fed into the LLM alongside the question to provide context for generating the final answer.\n",
    "\n",
    "The first step involves creating a Langchain retriever interface for a Raphtroy vectorised graph. To do this, we extend the `BaseRetriever` class and implement the `_get_relevant_documents` method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7e3496",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Document' from 'langchain.schema.retriever' (/Users/bensteer/miniconda3/envs/raphtory/lib/python3.11/site-packages/langchain/schema/retriever.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/bensteer/Documents/Raphtory/examples/py/enron/enron-vectors.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bensteer/Documents/Raphtory/examples/py/enron/enron-vectors.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretriever\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseRetriever, Document\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bensteer/Documents/Raphtory/examples/py/enron/enron-vectors.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanager\u001b[39;00m \u001b[39mimport\u001b[39;00m CallbackManagerForRetrieverRun\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bensteer/Documents/Raphtory/examples/py/enron/enron-vectors.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Dict, Any, List\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Document' from 'langchain.schema.retriever' (/Users/bensteer/miniconda3/envs/raphtory/lib/python3.11/site-packages/langchain/schema/retriever.py)"
     ]
    }
   ],
   "source": [
    "from langchain.schema.retriever import BaseRetriever, Document\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "def adapt_document(document):\n",
    "    return Document(\n",
    "        page_content=document.content,\n",
    "        metadata={\n",
    "            'src': document.entity.src.name,\n",
    "            'dst': document.entity.dst.name\n",
    "        }\n",
    "    )\n",
    "\n",
    "class RaphtoryRetriever(BaseRetriever):\n",
    "    graph: VectorisedGraph\n",
    "    \"\"\"Source graph.\"\"\"\n",
    "    top_k: Optional[int]\n",
    "    \"\"\"Number of items to return.\"\"\"\n",
    "    \n",
    "    def _get_relevant_documents(\n",
    "        self,\n",
    "        query: str,\n",
    "        *,\n",
    "        run_manager: CallbackManagerForRetrieverRun,\n",
    "        metadata: Optional[Dict[str, Any]] = None,\n",
    "    ) -> List[Document]:\n",
    "        docs = self.graph.append_edges_by_similarity(query, self.top_k).get_documents()\n",
    "        return [adapt_document(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab1b81",
   "metadata": {},
   "source": [
    "Next, we can define the RAG chain using our retriever as the context. In this instance, we'll create a placeholder LLM model that answres with the statement, `\"I'm a dummy LLM model that got the input:\"` and returns the input it receives. While this dummy model might not serve as a practical investigative tool, it allows us to observe the output from our retriever in action. If you do want to build a proper pipeline, you can replace the placeholder LLM with a real one using the code snippet beolw:\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "```\n",
    "\n",
    "This, however,  requires an OpenAI access token. For now, let's proceed with our dummy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f297e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retriever = RaphtoryRetriever(graph=vg, top_k=3)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = lambda input: f\"I'm a dummy LLM model that got the input:\\n{input.text}\"\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28876334",
   "metadata": {},
   "source": [
    "And finally we can invoke the chain providing a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd478c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.invoke('which person should I investigate to know more about Enron usage of special purpose entities')\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raphtory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
