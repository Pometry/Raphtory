#https://doc.akka.io/docs/akka/current/general/configuration-reference.html

Raphtory {
    builderServers        = 1
    partitionServers      = 3
    buildersPerServer     = 5
    partitionsPerServer   = 5
}

settings {
	hostname = ${?HOSTNAME}
	kubeip	 = ${?SEEDNODE_SERVICE_HOST} // original was localhost/
	ip       = ${?HOST_IP}
	http 		 = 8080
	#port 		 = 1400
	#port     = ${?HOST_PORT}
}
akka.cluster.jmx.multi-mbeans-in-same-jvm = on
akka.management.cluster.bootstrap.contact-point-discovery {
	discovery-method = kubernetes-api
}



akka.logger-startup-timeout = 30s
akka.actor.warn-about-java-serializer-usage=false
akka {
	log-dead-letters = 10
	log-dead-letters-during-shutdown = on
	extensions = ["akka.cluster.pubsub.DistributedPubSub"]
	loglevel = "INFO"
	loggers = ["akka.event.slf4j.Slf4jLogger"]
	stdout-loglevel = "INFO"
	#log-config-on-start = on


	discovery {
		method = kubernetes-api
		method = ${?AKKA_DISCOVERY_METHOD}
		kubernetes-api {
			pod-namespace = "default" // in which namespace cluster is running
			pod-namespace = ${?AKKA_NAMESPACE}
			pod-label-selector = "app=akka-simple-cluster" // selector - hot to find other cluster nodes
			pod-label-selector = ${?AKKA_POD_LABEL_SELECTOR}
			pod-port-name = "management" // name of cluster management port
			pod-port-name = ${?AKKA_MANAGEMENT_PORT_NAME}
		}
	}

	#actor.allow-java-serialization = on
	#actor.warn-about-java-serializer-usage = on

	actor {
		#provider = cluster
		provider = akka.cluster.ClusterActorRefProvider
		#serialize-messages = on
		serializers {
			proto = "akka.remote.serialization.ProtobufSerializer"
			#kryo = "io.altoo.akka.serialization.kryo.KryoSerializer"
			kryo = "com.twitter.chill.akka.AkkaSerializer"
		}
		debug {
			# enable function of LoggingReceive, which is to log any received message at
			# DEBUG level
			receive = off
		}
		serialization-bindings {
                "java.io.Serializable" = kryo
        }
	}
	bounded-mailbox {
		mailbox-type = "akka.dispatch.NonBlockingBoundedMailbox"
		mailbox-capacity = 200000000
	}

	actor.mailbox.requirements {
		"akka.dispatch.BoundedMessageQueueSemantics" = bounded-mailbox
	}
	remote {
		artery {
			transport = tcp
			//log-sent-messages = on#
			//log-received-messages = on
			//log-frame-size-exceeding = 1200000b
 			advanced {
				outbound-message-queue-size = 1000000
				maximum-frame-size = 2MB
				outbound-control-queue-size = 1000000
			}

		}
		artery.canonical {

			#bind-hostname = 0.0.0.0
			#bind-hostname = ${?HOST_IP}
			#bind-port     = ${settings.bport}

			hostname = ${?HOST_IP}
			hostname = ${?SEEDNODE_SERVICE_HOST}

			#port     = ${settings.port}
		}
	}

	failure-detector {

		# FQCN of the failure detector implementation.
		# It must implement akka.remote.FailureDetector and have
		# a public constructor with a com.typesafe.config.Config and
		# akka.actor.EventStream parameter.
		implementation-class = "akka.remote.PhiAccrualFailureDetector"

		# How often keep-alive heartbeat messages should be sent to each connection.
		heartbeat-interval = 10 s

		# Defines the failure detector threshold.
		# A low threshold is prone to generate many wrong suspicions but ensures
		# a quick detection in the event of a real crash. Conversely, a high
		# threshold generates fewer mistakes but needs more time to detect
		# actual crashes.
		threshold = 30

		# Number of the samples of inter-heartbeat arrival times to adaptively
		# calculate the failure timeout for connections.
		max-sample-size = 1000

		# Minimum standard deviation to use for the normal distribution in
		# AccrualFailureDetector. Too low standard deviation might result in
		# too much sensitivity for sudden, but normal, deviations in heartbeat
		# inter arrival times.
		min-std-deviation = 100 ms

		# Number of potentially lost/delayed heartbeats that will be
		# accepted before considering it to be an anomaly.
		# This margin is important to be able to survive sudden, occasional,
		# pauses in heartbeat arrivals, due to for example garbage collect or
		# network drop.
		acceptable-heartbeat-pause = 10 s

		# Number of member nodes that each member will send heartbeat messages to,
		# i.e. each node will be monitored by this number of other nodes.
		monitored-by-nr-of-members = 5

		# After the heartbeat request has been sent the first failure detection
		# will start after this period, even though no heartbeat message has
		# been received.
		expected-response-after = 1 s

	}
	cluster {
		seed-nodes = [
			# Set programatically (passed in on args list)
			# e.g.		"akka.tcp://ClusterSystem@127.0.0.1:2551"
		]
		auto-down-unreachable-after = 20m
	}
	scheduler {
		# The LightArrayRevolverScheduler is used as the default scheduler in the
		# system. It does not execute the scheduled tasks on exact time, but on every
		# tick, it will run everything that is (over)due. You can increase or decrease
		# the accuracy of the execution timing by specifying smaller or larger tick
		# duration. If you are scheduling a lot of tasks you should consider increasing
		# the ticks per wheel.
		# Note that it might take up to 1 tick to stop the Timer, so setting the
		# tick-duration to a high value will make shutting down the actor system
		# take longer.
		tick-duration = 10ms

		# The timer uses a circular wheel of buckets to store the timer tasks.
		# This should be set such that the majority of scheduled timeouts (for high
		# scheduling frequency) will be shorter than one rotation of the wheel
		# (ticks-per-wheel * ticks-duration)
		# THIS MUST BE A POWER OF TWO!
		ticks-per-wheel = 512

		# This setting selects the timer implementation which shall be loaded at
		# system start-up.
		# The class given here must implement the akka.actor.Scheduler interface
		# and offer a public constructor which takes three arguments:
		#  1) com.typesafe.config.Config
		#  2) akka.event.LoggingAdapter
		#  3) java.util.concurrent.ThreadFactory
		implementation = akka.actor.LightArrayRevolverScheduler

		# When shutting down the scheduler, there will typically be a thread which
		# needs to be stopped, and this timeout determines how long to wait for
		# that to happen. In case of timeout the shutdown of the actor system will
		# proceed without running possibly still enqueued tasks.
		shutdown-timeout = 5s
	}
}



worker-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	#mailbox-type = "com.raphtory.core.components.actor.SizeTrackedMailbox"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	throughput = 1
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
}

reader-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
builder-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
misc-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
analysis-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
spout-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 10
}
