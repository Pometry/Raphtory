#https://doc.akka.io/docs/akka/current/general/configuration-reference.html

Raphtory {
    builderServers        = 2
    builderServers        = ${?RAPHTORY_BUILD_SERVERS}

    partitionServers      = 2
    partitionServers      = ${?RAPHTORY_PARTITION_SERVERS}

    buildersPerServer     = 2
    buildersPerServer     = ${?RAPHTORY_BUILDERS_PER_SERVER}

    partitionsPerServer   = 4
    partitionsPerServer   = ${?RAPHTORY_PARTITIONS_PER_SERVER}

    leaderAddress         = "127.0.0.1"
    leaderAddress         = ${?RAPHTORY_LEADER_ADDRESS}

    leaderPort            = "1600"
    leaderPort            = ${?RAPHTORY_LEADER_PORT}

    bindAddress           = "127.0.0.1"
    bindAddress           = ${?RAPHTORY_BIND_ADDRESS}

    bindPort              = 1600
    bindPort              = ${?RAPHTORY_BIND_PORT}

    builderBatchSize      = 100
    builderBatchSize      = ${?RAPHTORY_BUILDER_BATCH_SIZE}

    builderMaxCache       = 1000
    builderMaxCache       = ${?RAPHTORY_BUILDER_MAX_CACHE}

    partitionMinQueue     = 100
    partitionMinQueue     = ${?RAPHTORY_PARTITION_MIN_QUEUE}

    hasDeletions          = true
    hasDeletions          = ${?RAPHTORY_DATA_HAS_DELETIONS}
}

settings {
	hostname = ${?HOSTNAME}
	kubeip	 = ${?SEEDNODE_SERVICE_HOST} // original was localhost/
	ip       = ${?HOST_IP}
	http 		 = 8080
	#port 		 = 1400
	#port     = ${?HOST_PORT}
}
akka.cluster.jmx.multi-mbeans-in-same-jvm = on
akka.management.cluster.bootstrap.contact-point-discovery {
	discovery-method = kubernetes-api
}



akka.logger-startup-timeout = 30s
akka.actor.warn-about-java-serializer-usage=false
akka.actor.mailbox.requirements {
    "com.raphtory.core.components.partitionmanager.Writer" = my-custom-mailbox
}
my-custom-mailbox {
    mailbox-type = "com.raphtory.core.components.akkamanagement.SizeTrackedMailbox"
}

akka {
	log-dead-letters = 10
	log-dead-letters-during-shutdown = on
	extensions = ["akka.cluster.pubsub.DistributedPubSub"]
	loglevel = "INFO"
	loggers = ["akka.event.slf4j.Slf4jLogger"]
	stdout-loglevel = "INFO"
	#log-config-on-start = on


	discovery {
		method = kubernetes-api
		method = ${?AKKA_DISCOVERY_METHOD}
		kubernetes-api {
			pod-namespace = "default" // in which namespace cluster is running
			pod-namespace = ${?AKKA_NAMESPACE}
			pod-label-selector = "app=akka-simple-cluster" // selector - hot to find other cluster nodes
			pod-label-selector = ${?AKKA_POD_LABEL_SELECTOR}
			pod-port-name = "management" // name of cluster management port
			pod-port-name = ${?AKKA_MANAGEMENT_PORT_NAME}
		}
	}

	#actor.allow-java-serialization = on
	#actor.warn-about-java-serializer-usage = on

	actor {
		#provider = cluster
		provider = akka.cluster.ClusterActorRefProvider
		#serialize-messages = on
		serializers {
			proto = "akka.remote.serialization.ProtobufSerializer"
			#kryo = "io.altoo.akka.serialization.kryo.KryoSerializer"
			kryo = "com.raphtory.core.components.akkamanagement.RaphtoryAkkaSerialiser"
		}
		debug {
			# enable function of LoggingReceive, which is to log any received message at
			# DEBUG level
			receive = off
		}
		serialization-bindings {
<<<<<<< HEAD
                "java.io.Serializable" = kryo
        }
=======
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$RequestRouterId$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$ClusterStatusRequest$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$RequestPartitionId$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$RequestSpoutId$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$RequestAnalysisId$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$PartitionsCount" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$ClusterStatusResponse" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$AssignedId" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$RouterUp" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$SpoutUp" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$AnalysisManagerUp" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$PartitionUp" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatchDog$Message$RequestPartitionCount$" = kryo

			"com.raphtory.core.actors.orchestration.clustermanager.WatermarkManager$Message$ProbeWatermark$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatermarkManager$Message$WatermarkTime$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatermarkManager$Message$SaveState$" = kryo
			"com.raphtory.core.actors.orchestration.clustermanager.WatermarkManager$Message$WatermarkTime" = kryo

			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$SpoutOnline$" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$WorkPlease$" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$AllocateTuple" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$DataFinished$" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$NoWork$" = kryo

			"com.raphtory.core.actors.graphbuilder.RouterWorker$CommonMessage$RouterWorkerTimeSync" = kryo
			"com.raphtory.core.actors.graphbuilder.RouterWorker$CommonMessage$DataFinishedSync" = kryo

			"com.raphtory.core.model.communication.TrackedGraphUpdate" = kryo
			"com.raphtory.core.model.communication.VertexAdd" = kryo
			"com.raphtory.core.model.communication.EdgeAdd" = kryo
			"com.raphtory.core.model.communication.VertexDelete" = kryo
			"com.raphtory.core.model.communication.EdgeDelete" = kryo
			"com.raphtory.core.model.communication.Properties" = kryo
			"com.raphtory.core.model.communication.DoubleProperty" = kryo
			"com.raphtory.core.model.communication.ImmutableProperty" = kryo
			"com.raphtory.core.model.communication.StringProperty" = kryo
			"com.raphtory.core.model.communication.LongProperty" = kryo
			"com.raphtory.core.model.communication.DoubleProperty" = kryo
			"com.raphtory.core.model.communication.Type" = kryo

			"com.raphtory.core.model.communication.TrackedGraphEffect" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeAdd" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeRemoval" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeRemovalFromVertex" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeAddNew" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeRemovalNew" = kryo
			"com.raphtory.core.model.communication.RemoteReturnDeaths" = kryo
			"com.raphtory.core.model.communication.ReturnEdgeRemoval" = kryo

			"com.raphtory.core.model.communication.DstAddForOtherWorker" = kryo
			"com.raphtory.core.model.communication.DstWipeForOtherWorker" = kryo
			"com.raphtory.core.model.communication.DstResponseFromOtherWorker" = kryo
			"com.raphtory.core.model.communication.EdgeRemoveForOtherWorker" = kryo
			"com.raphtory.core.model.communication.EdgeSyncAck" = kryo
			"com.raphtory.core.model.communication.VertexRemoveSyncAck" = kryo

			"com.raphtory.core.model.communication.VertexMessage" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartAnalysis$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReaderWorkersOnline$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReaderWorkersAck$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$CompileNewAnalyser" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$LoadPredefinedAnalyser" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$FailedToCompile" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$AnalyserPresent" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$TimeCheck$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$TimeResponse" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$RecheckTime$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupSubtask" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupSubtaskDone$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartSubtask" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$Ready" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupNextStep" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupNextStepDone$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartNextStep" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$CheckMessages" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$MessagesReceived" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$EndStep" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$Finish" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReturnResults" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartNextSubtask$" = kryo
			"com.raphtory.core.actors.analysismanager.AnalysisManager$Message$KillTask" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$JobKilled$" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$ResultsForApiPI" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$JobDoesntExist$" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$ManagingTask" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$AreYouFinished$" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$TaskFinished" = kryo
            "com.raphtory.core.actors.analysismanager.AnalysisManager$Message$JobFailed$" = kryo

			#"scala.collection.mutable.TreeMap" = kryo
			#"scala.None$" = kryo
			#"scala.collection.immutable.$colon$colon" = kryo

            "com.raphtory.algorithms.PartitionState"=kryo
		}

>>>>>>> master
	}
	bounded-mailbox {
		mailbox-type = "akka.dispatch.NonBlockingBoundedMailbox"
		mailbox-capacity = 200000000
	}

	actor.mailbox.requirements {
		"akka.dispatch.BoundedMessageQueueSemantics" = bounded-mailbox
	}
	remote {
		artery {
			transport = tcp
			#//log-sent-messages = on#
			#//log-received-messages = on
			#//log-frame-size-exceeding = 1200000b
 			advanced {
 			    idle-cpu-level = 5
				#outbound-message-queue-size = 500000
				outbound-large-message-queue-size = 100000
		        maximum-frame-size = 256 KiB
                maximum-large-frame-size = 10 MiB
			    #outbound-control-queue-size = 500000
			}
            large-message-destinations = ["/user/*"]

		}
		artery.canonical {

			#bind-hostname = 0.0.0.0
			#bind-port     = ${settings.bport}
			#bind-hostname = ${?HOST_IP}


			hostname = ${?HOST_IP}
			hostname = ${?SEEDNODE_SERVICE_HOST}

			#port     = ${settings.port}
		}
	}

	cluster.failure-detector {

		# FQCN of the failure detector implementation.
		# It must implement akka.remote.FailureDetector and have
		# a public constructor with a com.typesafe.config.Config and
		# akka.actor.EventStream parameter.
		implementation-class = "akka.remote.PhiAccrualFailureDetector"

		# How often keep-alive heartbeat messages should be sent to each connection.
		heartbeat-interval = 10 s

		# Defines the failure detector threshold.
		# A low threshold is prone to generate many wrong suspicions but ensures
		# a quick detection in the event of a real crash. Conversely, a high
		# threshold generates fewer mistakes but needs more time to detect
		# actual crashes.
		threshold = 10000

		# Number of the samples of inter-heartbeat arrival times to adaptively
		# calculate the failure timeout for connections.
		max-sample-size = 10000

		# Minimum standard deviation to use for the normal distribution in
		# AccrualFailureDetector. Too low standard deviation might result in
		# too much sensitivity for sudden, but normal, deviations in heartbeat
		# inter arrival times.
		min-std-deviation = 12000 ms

		# Number of potentially lost/delayed heartbeats that will be
		# accepted before considering it to be an anomaly.
		# This margin is important to be able to survive sudden, occasional,
		# pauses in heartbeat arrivals, due to for example garbage collect or
		# network drop.
		acceptable-heartbeat-pause = 100 s

		# Number of member nodes that each member will send heartbeat messages to,
		# i.e. each node will be monitored by this number of other nodes.
		monitored-by-nr-of-members = 2

		# After the heartbeat request has been sent the first failure detection
		# will start after this period, even though no heartbeat message has
		# been received.
		expected-response-after = 15 s

	}

	cluster {
		seed-nodes = []
		auto-down-unreachable-after = 20m
	}
	scheduler {
		# The LightArrayRevolverScheduler is used as the default scheduler in the
		# system. It does not execute the scheduled tasks on exact time, but on every
		# tick, it will run everything that is (over)due. You can increase or decrease
		# the accuracy of the execution timing by specifying smaller or larger tick
		# duration. If you are scheduling a lot of tasks you should consider increasing
		# the ticks per wheel.
		# Note that it might take up to 1 tick to stop the Timer, so setting the
		# tick-duration to a high value will make shutting down the actor system
		# take longer.
		tick-duration = 10ms

		# The timer uses a circular wheel of buckets to store the timer tasks.
		# This should be set such that the majority of scheduled timeouts (for high
		# scheduling frequency) will be shorter than one rotation of the wheel
		# (ticks-per-wheel * ticks-duration)
		# THIS MUST BE A POWER OF TWO!
		ticks-per-wheel = 512

		# This setting selects the timer implementation which shall be loaded at
		# system start-up.
		# The class given here must implement the akka.actor.Scheduler interface
		# and offer a public constructor which takes three arguments:
		#  1) com.typesafe.config.Config
		#  2) akka.event.LoggingAdapter
		#  3) java.util.concurrent.ThreadFactory
		implementation = akka.actor.LightArrayRevolverScheduler

		# When shutting down the scheduler, there will typically be a thread which
		# needs to be stopped, and this timeout determines how long to wait for
		# that to happen. In case of timeout the shutdown of the actor system will
		# proceed without running possibly still enqueued tasks.
		shutdown-timeout = 5s
	}
}


kamon {

	akka.actor-groups = [ "Router","PM-Children" ]
	util.filters {
		"akka.tracked-actor" {
			includes = ["dockerexp/user/Manager_*","dockerexp/user/router","dockerexp/user/UpdateGen", "dockerexp/user/Manager_*_child_*","dockerexp/user/router/child_*"],
			excludes = ["dockerexp/system/**"]
		}
		actors.track {
			includes = [ "dockerexp/user/*" ]
			excludes = []
		}

		"PM-Children" {
			includes = [ "dockerexp/user/Manager_*_child_*" ]
		}
		"Router" {
			includes = [ "dockerexp/user/router/child_*" ]
		}
	}

	prometheus {
		embedded-server {
			# Hostname and port used by the embedded web server to publish the scraping enpoint.
			hostname = 0.0.0.0
			port = 11600
		}
		buckets {
			default-buckets = [
				10,
				30,
				100,
				300,
				1000,
				3000,
				10000,
				30000,
				100000
			]

			time-buckets = [
				0.005,
				0.01,
				0.025,
				0.05,
				0.075,
				0.1,
				0.25,
				0.5,
				0.75,
				1,
				2.5,
				5,
				7.5,
				10
			]

			information-buckets = [
				512,
				1024,
				2048,
				4096,
				16384,
				65536,
				524288,
				1048576
			]
		}
	}
}

>>>>>>> master
worker-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	#mailbox-requirement = my-custom-mailbox
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	throughput = 1
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
}

reader-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}

builder-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}

misc-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
analysis-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
spout-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 10
}
