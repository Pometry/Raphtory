settings {

//  hostname = ${?HOSTNAME}
//	ip		   = ${?HOST_IP} // original was localhost/
//	externalip = ${SEEDNODE_SERVICE_HOST}
//=======
	hostname = ${?HOSTNAME}
	kubeip	 = ${?SEEDNODE_SERVICE_HOST} // original was localhost/
	ip       = ${?HOST_IP}
//>>>>>>> e543c2050359453e428f41c0cfe96cc60aa37521
	//hostName = "localhost"
	//ip = 0.0.0.0
	http 		 = 8080
	port 		 = 1600
	port     = ${?HOST_PORT}
	bport		 = 1600
	bport    = ${?HOST_PORT}
}
akka.cluster.jmx.multi-mbeans-in-same-jvm = on
akka.management.cluster.bootstrap.contact-point-discovery {
	discovery-method = kubernetes-api
}

akka.logger-startup-timeout = 30s
akka.actor.warn-about-java-serializer-usage=false
akka {
	log-dead-letters = 10
	log-dead-letters-during-shutdown = on
	extensions = ["akka.cluster.pubsub.DistributedPubSub"]
	loglevel = "ERROR"
	loggers = ["akka.event.slf4j.Slf4jLogger"]
	stdout-loglevel = "ERROR"
	#log-config-on-start = on


	discovery {
		method = kubernetes-api
		method = ${?AKKA_DISCOVERY_METHOD}
		kubernetes-api {
			pod-namespace = "default" // in which namespace cluster is running
			pod-namespace = ${?AKKA_NAMESPACE}
			pod-label-selector = "app=akka-simple-cluster" // selector - hot to find other cluster nodes
			pod-label-selector = ${?AKKA_POD_LABEL_SELECTOR}
			pod-port-name = "management" // name of cluster management port
			pod-port-name = ${?AKKA_MANAGEMENT_PORT_NAME}
		}
	}

	//actor.allow-java-serialization = on
//	actor.warn-about-java-serializer-usage = off

	actor {
		#provider = cluster
		provider = akka.cluster.ClusterActorRefProvider
		#serialize-messages = on
		serializers {
			proto = "akka.remote.serialization.ProtobufSerializer"
			kryo = "io.altoo.akka.serialization.kryo.KryoSerializer"
		}
		debug {
			# enable function of LoggingReceive, which is to log any received message at
			# DEBUG level
			receive = off
		}
		serialization-bindings {
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RequestRouterId$" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$ClusterStatusRequest$" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RequestPartitionId$" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$PartitionsCount" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$ClusterStatusResponse" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$AssignedId" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RouterUp" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$PartitionUp" = kryo
			"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RequestPartitionCount$" = kryo

			"com.raphtory.core.actors.clustermanagement.WatermarkManager$Message$ProbeWatermark$" = kryo
			"com.raphtory.core.actors.clustermanagement.WatermarkManager$Message$WatermarkTime$" = kryo
			"com.raphtory.core.actors.clustermanagement.WatermarkManager$Message$WatermarkTime" = kryo

			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$SpoutOnline$" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$WorkPlease$" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$AllocateTuple" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$DataFinished$" = kryo
			"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$NoWork$" = kryo

			"com.raphtory.core.actors.graphbuilder.RouterWorker$CommonMessage$RouterWorkerTimeSync" = kryo
			"com.raphtory.core.actors.graphbuilder.RouterWorker$CommonMessage$DataFinishedSync" = kryo

			"com.raphtory.core.model.communication.TrackedGraphUpdate" = kryo
			"com.raphtory.core.model.communication.VertexAdd" = kryo
			"com.raphtory.core.model.communication.EdgeAdd" = kryo
			"com.raphtory.core.model.communication.VertexDelete" = kryo
			"com.raphtory.core.model.communication.EdgeDelete" = kryo
			"com.raphtory.core.model.communication.Properties" = kryo
			"com.raphtory.core.model.communication.DoubleProperty" = kryo
			"com.raphtory.core.model.communication.ImmutableProperty" = kryo
			"com.raphtory.core.model.communication.StringProperty" = kryo
			"com.raphtory.core.model.communication.LongProperty" = kryo
			"com.raphtory.core.model.communication.DoubleProperty" = kryo
			"com.raphtory.core.model.communication.Type" = kryo

			"com.raphtory.core.model.communication.TrackedGraphEffect" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeAdd" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeRemoval" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeRemovalFromVertex" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeAddNew" = kryo
			"com.raphtory.core.model.communication.RemoteEdgeRemovalNew" = kryo
			"com.raphtory.core.model.communication.RemoteReturnDeaths" = kryo
			"com.raphtory.core.model.communication.ReturnEdgeRemoval" = kryo

			"com.raphtory.core.model.communication.DstAddForOtherWorker" = kryo
			"com.raphtory.core.model.communication.DstWipeForOtherWorker" = kryo
			"com.raphtory.core.model.communication.DstResponseFromOtherWorker" = kryo
			"com.raphtory.core.model.communication.EdgeRemoveForOtherWorker" = kryo
			"com.raphtory.core.model.communication.EdgeSyncAck" = kryo
			"com.raphtory.core.model.communication.VertexRemoveSyncAck" = kryo

			"com.raphtory.core.model.communication.VertexMessage" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartAnalysis$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReaderWorkersOnline$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReaderWorkersAck$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$CompileNewAnalyser" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$LoadPredefinedAnalyser" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$FailedToCompile" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$AnalyserPresent" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$TimeCheck$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$TimeResponse" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$RecheckTime$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupSubtask" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupSubtaskDone$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartSubtask" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$Ready" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupNextStep" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupNextStepDone$" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartNextStep" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$CheckMessages" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$MessagesReceived" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$EndStep" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$Finish" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReturnResults" = kryo
			"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartNextSubtask$" = kryo
			"com.raphtory.core.actors.analysismanager.AnalysisManager$Message$KillTask" = kryo
			#"scala.collection.mutable.TreeMap" = kryo
			#"scala.None$" = kryo
			#"scala.collection.immutable.$colon$colon" = kryo

		}

	}
	bounded-mailbox {
		mailbox-type = "akka.dispatch.NonBlockingBoundedMailbox"
		mailbox-capacity = 200000000
	}

	actor.mailbox.requirements {
		"akka.dispatch.BoundedMessageQueueSemantics" = bounded-mailbox
	}

	remote {
		artery {
			transport = tcp
			//log-sent-messages = on#
			//log-received-messages = on
			//log-frame-size-exceeding = 1200000b
 			advanced {
				outbound-message-queue-size = 1000000

				outbound-control-queue-size = 1000000
			}

		}
		artery.canonical {

			bind-hostname = 0.0.0.0
			bind-hostname = ${?HOST_IP}
			bind-port     = ${settings.bport}

			hostname = ${?HOST_IP}
			hostname = ${?SEEDNODE_SERVICE_HOST}

			port     = ${settings.port}
			#send-buffer-size 		= 256000000b
			#receive-buffer-size = 256000000b
			#maximum-frame-size  = 128000000b
		}
	}

	failure-detector {

		# FQCN of the failure detector implementation.
		# It must implement akka.remote.FailureDetector and have
		# a public constructor with a com.typesafe.config.Config and
		# akka.actor.EventStream parameter.
		implementation-class = "akka.remote.PhiAccrualFailureDetector"

		# How often keep-alive heartbeat messages should be sent to each connection.
		heartbeat-interval = 1 s

		# Defines the failure detector threshold.
		# A low threshold is prone to generate many wrong suspicions but ensures
		# a quick detection in the event of a real crash. Conversely, a high
		# threshold generates fewer mistakes but needs more time to detect
		# actual crashes.
		threshold = 30

		# Number of the samples of inter-heartbeat arrival times to adaptively
		# calculate the failure timeout for connections.
		max-sample-size = 1000

		# Minimum standard deviation to use for the normal distribution in
		# AccrualFailureDetector. Too low standard deviation might result in
		# too much sensitivity for sudden, but normal, deviations in heartbeat
		# inter arrival times.
		min-std-deviation = 100 ms

		# Number of potentially lost/delayed heartbeats that will be
		# accepted before considering it to be an anomaly.
		# This margin is important to be able to survive sudden, occasional,
		# pauses in heartbeat arrivals, due to for example garbage collect or
		# network drop.
		acceptable-heartbeat-pause = 10 s

		# Number of member nodes that each member will send heartbeat messages to,
		# i.e. each node will be monitored by this number of other nodes.
		monitored-by-nr-of-members = 5

		# After the heartbeat request has been sent the first failure detection
		# will start after this period, even though no heartbeat message has
		# been received.
		expected-response-after = 1 s

	}
	cluster {
		seed-nodes = [
			# Set programatically (passed in on args list)
			# e.g.		"akka.tcp://ClusterSystem@127.0.0.1:2551"
		]
		auto-down-unreachable-after = 20m
	}
	scheduler {
		# The LightArrayRevolverScheduler is used as the default scheduler in the
		# system. It does not execute the scheduled tasks on exact time, but on every
		# tick, it will run everything that is (over)due. You can increase or decrease
		# the accuracy of the execution timing by specifying smaller or larger tick
		# duration. If you are scheduling a lot of tasks you should consider increasing
		# the ticks per wheel.
		# Note that it might take up to 1 tick to stop the Timer, so setting the
		# tick-duration to a high value will make shutting down the actor system
		# take longer.
		tick-duration = 10ms

		# The timer uses a circular wheel of buckets to store the timer tasks.
		# This should be set such that the majority of scheduled timeouts (for high
		# scheduling frequency) will be shorter than one rotation of the wheel
		# (ticks-per-wheel * ticks-duration)
		# THIS MUST BE A POWER OF TWO!
		ticks-per-wheel = 512

		# This setting selects the timer implementation which shall be loaded at
		# system start-up.
		# The class given here must implement the akka.actor.Scheduler interface
		# and offer a public constructor which takes three arguments:
		#  1) com.typesafe.config.Config
		#  2) akka.event.LoggingAdapter
		#  3) java.util.concurrent.ThreadFactory
		implementation = akka.actor.LightArrayRevolverScheduler

		# When shutting down the scheduler, there will typically be a thread which
		# needs to be stopped, and this timeout determines how long to wait for
		# that to happen. In case of timeout the shutdown of the actor system will
		# proceed without running possibly still enqueued tasks.
		shutdown-timeout = 5s
	}
}

akka-kryo-serialization {

	# Possible values for id-strategy are:
	# default, explicit, incremental, automatic
	id-strategy = "explicit"
	implicit-registration-logging = false

	# Define a default size for byte buffers used during serialization
	buffer-size = 64000

	# The serialization byte buffers are doubled as needed until they exceed
	# maxBufferSize and an exception is thrown. Can be -1 for no maximum.
	max-buffer-size = -1

	# To use a custom queue the [[io.altoo.akka.serialization.kryo.DefaultQueueBuilder]]
	# can be extended and registered here.
	#queue-builder = "io.altoo.akka.serialization.kryo.DefaultQueueBuilder"

	# If set, akka uses manifests to put a class name
	# of the top-level object into each message
	use-manifests = false


	# If enabled, Kryo logs a lot of information about serialization process.
	# Useful for debugging and low-level tweaking
	kryo-trace = false

	# If enabled, Kryo uses internally a map detecting shared nodes.
	# This is a preferred mode for big object graphs with a lot of nodes.
	# For small object graphs (e.g. below 10 nodes) set it to false for
	# better performance.
	kryo-reference-map = true

	# If enabled, allows Kryo to resolve subclasses of registered Types.
	#
	# This is primarily useful when id-strategy is set to "explicit". In this
	# case, all classes to be serialized must be explicitly registered. The
	# problem is that a large number of common Scala and Akka types (such as
	# Map and ActorRef) are actually traits that mask a large number of
	# specialized classes that deal with various situations and optimizations.
	# It isn't straightforward to register all of these, so you can instead
	# register a single supertype, with a serializer that can handle *all* of
	# the subclasses, and the subclasses get serialized with that.
	#
	# Use this with care: you should only rely on this when you are confident
	# that the superclass serializer covers all of the special cases properly.
	resolve-subclasses = false

	# Define mappings from a fully qualified class name to a numeric id.
	# Using ids instead of FQCN leads to smaller sizes of serialized representations
	# and faster serialization.
	#
	# This section is mandatory for idstartegy=explicit
	# This section is optional  for idstartegy=incremental
	# This section is ignored   for idstartegy=default
	#
	# The smallest possible id should start at 20 (or even higher), because
	# ids below it are used by Kryo internally e.g. for built-in Java and
	# Scala types.
	#
	# Some helpful mappings are provided through `supplied-basic-mappings`
	# and can be added/extended by:
	#
	 mappings =  {
		 "java.util.UUID" = 30

		 "java.time.LocalDate" = 31
		 "java.time.LocalDateTime" = 32
		 "java.time.LocalTime" = 33
		 "java.time.ZoneOffset" = 34
		 "java.time.ZoneRegion" = 35
		 "java.time.ZonedDateTime" = 36
		 "java.time.Instant" = 37
		 "java.time.Duration" = 38

		 // scala
		 "scala.Some" = 50
		 "scala.None$" = 51
		 "scala.util.Left" = 52
		 "scala.util.Right" = 53
		 "scala.util.Success" = 54
		 "scala.util.Failure" = 55

		 "scala.Tuple2" = 60
		 "scala.Tuple3" = 61
		 "scala.Tuple4" = 62
		 "scala.Tuple5" = 63
		 "scala.Tuple6" = 64
		 "scala.Tuple7" = 65
		 "scala.Tuple8" = 66
		 "scala.collection.immutable.Nil$" = 70
		 "scala.collection.immutable.$colon$colon" = 71
		 "scala.collection.immutable.Map$EmptyMap$" = 72
		 "scala.collection.immutable.Map$Map1" = 73
		 "scala.collection.immutable.Map$Map2" = 74
		 "scala.collection.immutable.Map$Map3" = 75
		 "scala.collection.immutable.Map$Map4" = 76
		 "scala.collection.immutable.Set$EmptySet$" = 77
		 "scala.collection.immutable.Set$Set1" = 78
		 "scala.collection.immutable.Set$Set2" = 79
		 "scala.collection.immutable.Set$Set3" = 80
		 "scala.collection.immutable.Set$Set4" = 81
		 "scala.Option$" = 82

	 }
	#
	#mappings {
		# fully.qualified.classname1 = id1
		# fully.qualified.classname2 = id2
	#}

	# Define a set of fully qualified class names for
	# classes to be used for serialization.
	# The ids for those classes will be assigned automatically,
	# but respecting the order of declaration in this section
	#
	# This section is optional  for idstartegy=incremental
	# This section is ignored   for idstartegy=default
	# This section is optional  for idstartegy=explicit
	classes = [
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RequestRouterId$"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$ClusterStatusRequest$"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RequestPartitionId$"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$PartitionsCount"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$ClusterStatusResponse"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$AssignedId"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RouterUp"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$PartitionUp"
		"com.raphtory.core.actors.clustermanagement.WatchDog$Message$RequestPartitionCount$"


		"com.raphtory.core.actors.clustermanagement.WatermarkManager$Message$ProbeWatermark$"
		"com.raphtory.core.actors.clustermanagement.WatermarkManager$Message$WatermarkTime$"
		"com.raphtory.core.actors.clustermanagement.WatermarkManager$Message$WatermarkTime"


		"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$SpoutOnline$"
		"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$WorkPlease$"
		"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$AllocateTuple"
		"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$DataFinished$"
		"com.raphtory.core.actors.spout.SpoutAgent$CommonMessage$NoWork$"

		"com.raphtory.core.actors.graphbuilder.RouterWorker$CommonMessage$RouterWorkerTimeSync"
		"com.raphtory.core.actors.graphbuilder.RouterWorker$CommonMessage$DataFinishedSync"

		"com.raphtory.core.model.communication.TrackedGraphUpdate"
		"com.raphtory.core.model.communication.VertexAdd"
		"com.raphtory.core.model.communication.EdgeAdd"
		"com.raphtory.core.model.communication.VertexDelete"
		"com.raphtory.core.model.communication.EdgeDelete"
		"com.raphtory.core.model.communication.Properties"
		"com.raphtory.core.model.communication.DoubleProperty"
		"com.raphtory.core.model.communication.ImmutableProperty"
		"com.raphtory.core.model.communication.StringProperty"
		"com.raphtory.core.model.communication.LongProperty"
		"com.raphtory.core.model.communication.DoubleProperty"
		"com.raphtory.core.model.communication.FloatProperty"
		"com.raphtory.core.model.communication.Type"

		"com.raphtory.core.model.communication.TrackedGraphEffect"
		"com.raphtory.core.model.communication.RemoteEdgeAdd"
		"com.raphtory.core.model.communication.RemoteEdgeRemoval"
		"com.raphtory.core.model.communication.RemoteEdgeRemovalFromVertex"
		"com.raphtory.core.model.communication.RemoteEdgeAddNew"
		"com.raphtory.core.model.communication.RemoteEdgeRemovalNew"
		"com.raphtory.core.model.communication.RemoteReturnDeaths"
		"com.raphtory.core.model.communication.ReturnEdgeRemoval"

		"com.raphtory.core.model.communication.DstAddForOtherWorker"
		"com.raphtory.core.model.communication.DstWipeForOtherWorker"
		"com.raphtory.core.model.communication.DstResponseFromOtherWorker"
		"com.raphtory.core.model.communication.EdgeRemoveForOtherWorker"
		"com.raphtory.core.model.communication.EdgeSyncAck"
		"com.raphtory.core.model.communication.VertexRemoveSyncAck"

		"com.raphtory.core.model.communication.VertexMessage"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartAnalysis$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReaderWorkersOnline$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReaderWorkersAck$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$CompileNewAnalyser"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$LoadPredefinedAnalyser"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$FailedToCompile"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$AnalyserPresent"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$TimeCheck$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$TimeResponse"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$RecheckTime$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupSubtask"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupSubtaskDone$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartSubtask"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$Ready"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupNextStep"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$SetupNextStepDone$"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartNextStep"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$CheckMessages"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$MessagesReceived"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$EndStep"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$Finish"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$ReturnResults"
		"com.raphtory.core.actors.analysismanager.tasks.AnalysisTask$Message$StartNextSubtask$"
		"com.raphtory.core.actors.analysismanager.AnalysisManager$Message$KillTask"

		"scala.collection.mutable.TreeMap"
		"scala.None$"
		"scala.collection.immutable.$colon$colon"
		"scala.Tuple2"
		"scala.Array"
		"scala.collection.mutable.WrappedArray$ofRef"
		"scala.Tuple9"
		"scala.Tuple10"
		"scala.Tuple11"
		"scala.Tuple12"
		"scala.Tuple13"
		"scala.Tuple14"
		"scala.Tuple15"
		"scala.Tuple16"
		"scala.Tuple17"
		"scala.Tuple18"
		"scala.Tuple19"
		"scala.Tuple20"
		"scala.Tuple21"
		"scala.Tuple22"
		"scala.collection.mutable.ArrayBuffer"
		"akka.actor.RepointableActorRef"
		"akka.remote.RemoteActorRef"
		"scala.Tuple2$mcII$sp"
		"scala.Tuple2$mcJI$sp"
		"scala.collection.mutable.HashMap"
	]

	# Note: even though only to be helpful, these mappings are considered
	# part of the api and changes are to be considered breaking the api
	optional-basic-mappings {
		// java

		#"scala.collection.immutable.ArraySeq$ofRef" = 82
		#"scala.collection.immutable.ArraySeq$ofInt" = 83
		#"scala.collection.immutable.ArraySeq$ofDouble" = 84
		#"scala.collection.immutable.ArraySeq$ofLong" = 85
		#"scala.collection.immutable.ArraySeq$ofFloat" = 86
		#"scala.collection.immutable.ArraySeq$ofChar" = 87
		#"scala.collection.immutable.ArraySeq$ofByte" = 88
		#"scala.collection.immutable.ArraySeq$ofShort" = 89
		#"scala.collection.immutable.ArraySeq$ofBoolean" = 90
		#"scala.collection.immutable.ArraySeq$ofUnit" = 91
	}



}


kamon {

	akka.actor-groups = [ "Router","PM-Children" ]
	util.filters {
		"akka.tracked-actor" {
			includes = ["dockerexp/user/Manager_*","dockerexp/user/router","dockerexp/user/UpdateGen", "dockerexp/user/Manager_*_child_*","dockerexp/user/router/child_*"],
			excludes = ["dockerexp/system/**"]
		}
		actors.track {
			includes = [ "dockerexp/user/*" ]
			excludes = []
		}

		"PM-Children" {
			includes = [ "dockerexp/user/Manager_*_child_*" ]
		}
		"Router" {
			includes = [ "dockerexp/user/router/child_*" ]
		}
	}

	prometheus {
		embedded-server {
			# Hostname and port used by the embedded web server to publish the scraping enpoint.
			hostname = 0.0.0.0
			port = 11600
		}
		buckets {
			default-buckets = [
				10,
				30,
				100,
				300,
				1000,
				3000,
				10000,
				30000,
				100000
			]

			time-buckets = [
				0.005,
				0.01,
				0.025,
				0.05,
				0.075,
				0.1,
				0.25,
				0.5,
				0.75,
				1,
				2.5,
				5,
				7.5,
				10
			]

			information-buckets = [
				512,
				1024,
				2048,
				4096,
				16384,
				65536,
				524288,
				1048576
			]
		}
	}
}

worker-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	throughput = 1
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
}
reader-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
router-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
misc-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
analysis-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
spout-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 10
}