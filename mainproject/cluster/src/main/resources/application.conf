settings {

//  hostname = ${?HOSTNAME}
//	ip		   = ${?HOST_IP} // original was localhost/
//	externalip = ${SEEDNODE_SERVICE_HOST}
//=======
	hostname = ${?HOSTNAME}
	kubeip	 = ${?SEEDNODE_SERVICE_HOST} // original was localhost/
	ip       = ${?HOST_IP}
//>>>>>>> e543c2050359453e428f41c0cfe96cc60aa37521
	//hostName = "localhost"
	//ip = 0.0.0.0
	http 		 = 8080
	port 		 = 1600
	bport		 = 1600
}

akka.management.cluster.bootstrap.contact-point-discovery {
	discovery-method = kubernetes-api
}

akka {
	log-dead-letters = 10
	log-dead-letters-during-shutdown = on
	extensions = ["akka.cluster.pubsub.DistributedPubSub"]
	loglevel = "INFO"
	stdout-loglevel = "INFO"
	loggers = ["akka.event.slf4j.Slf4jLogger"]

	discovery {
		method = kubernetes-api
		method = ${?AKKA_DISCOVERY_METHOD}
		kubernetes-api {
			pod-namespace = "default" // in which namespace cluster is running
			pod-namespace = ${?AKKA_NAMESPACE}
			pod-label-selector = "app=akka-simple-cluster" // selector - hot to find other cluster nodes
			pod-label-selector = ${?AKKA_POD_LABEL_SELECTOR}
			pod-port-name = "management" // name of cluster management port
			pod-port-name = ${?AKKA_MANAGEMENT_PORT_NAME}
		}
	}


	actor {
		#provider = cluster
		provider = akka.cluster.ClusterActorRefProvider
		#serialize-messages = on
		serializers {
			java = "akka.serialization.JavaSerializer"
			proto = "akka.remote.serialization.ProtobufSerializer"
		}

		serialization-bindings {
			"com.raphtory.core.model.communication.VertexMessage" = java
		}
		#	"java.lang.String" = java
		#		"docs.serialization.Customer" = java
		#			"com.google.protobuf.Message" = proto
		#	"docs.serialization.MyOwnSerializable" = myown
		#	"java.lang.Boolean" = myown
		#}
		#prio-dispatcher {
		#	type = "Dispatcher"
		#	mailbox-type = "com.gwz.dockerexp.Actors.ClusterActors.PriorityMailbox"
		#}
	}
	bounded-mailbox {
		mailbox-type = "akka.dispatch.NonBlockingBoundedMailbox"
		mailbox-capacity = 2000000
	}

	akka.actor.mailbox.requirements {
		"akka.dispatch.BoundedMessageQueueSemantics" = bounded-mailbox
	}

	remote {
		enabled-transports = ["akka.remote.netty.tcp"]
		netty.tcp {

			# Internal Docker
			#bind-hostname = ${settings.ip} # This cannot be left as localhost/127.0.0.1!  Reset this value in code to internal IP.
			#bind-hostname = "0.0.0.0"
			bind-hostname = 0.0.0.0
			bind-hostname = ${?HOST_IP}
			bind-port     = ${settings.bport}
			# External Docker addr
//<<<<<<< HEAD
//			hostname = ${settings.externalip}
//=======
			hostname = ${?HOST_IP}
			hostname = ${?SEEDNODE_SERVICE_HOST}
			#hostname = ${POD_NAME}"."${SERVICE_NAME}"."${NAMESPACE}".svc.cluster.local"
//>>>>>>> e543c2050359453e428f41c0cfe96cc60aa37521
			port     = ${settings.port}
			send-buffer-size 		= 256000000b
			receive-buffer-size = 256000000b
			maximum-frame-size  = 128000000b
		}
	}

	failure-detector {

		# FQCN of the failure detector implementation.
		# It must implement akka.remote.FailureDetector and have
		# a public constructor with a com.typesafe.config.Config and
		# akka.actor.EventStream parameter.
		implementation-class = "akka.remote.PhiAccrualFailureDetector"

		# How often keep-alive heartbeat messages should be sent to each connection.
		heartbeat-interval = 1 s

		# Defines the failure detector threshold.
		# A low threshold is prone to generate many wrong suspicions but ensures
		# a quick detection in the event of a real crash. Conversely, a high
		# threshold generates fewer mistakes but needs more time to detect
		# actual crashes.
		threshold = 30

		# Number of the samples of inter-heartbeat arrival times to adaptively
		# calculate the failure timeout for connections.
		max-sample-size = 1000

		# Minimum standard deviation to use for the normal distribution in
		# AccrualFailureDetector. Too low standard deviation might result in
		# too much sensitivity for sudden, but normal, deviations in heartbeat
		# inter arrival times.
		min-std-deviation = 100 ms

		# Number of potentially lost/delayed heartbeats that will be
		# accepted before considering it to be an anomaly.
		# This margin is important to be able to survive sudden, occasional,
		# pauses in heartbeat arrivals, due to for example garbage collect or
		# network drop.
		acceptable-heartbeat-pause = 10 s

		# Number of member nodes that each member will send heartbeat messages to,
		# i.e. each node will be monitored by this number of other nodes.
		monitored-by-nr-of-members = 5

		# After the heartbeat request has been sent the first failure detection
		# will start after this period, even though no heartbeat message has
		# been received.
		expected-response-after = 1 s

	}

	cluster {
		seed-nodes = [
			# Set programatically (passed in on args list)
			# e.g.		"akka.tcp://ClusterSystem@127.0.0.1:2551"
		]
		auto-down-unreachable-after = 20m
	}

	scheduler {
		# The LightArrayRevolverScheduler is used as the default scheduler in the
		# system. It does not execute the scheduled tasks on exact time, but on every
		# tick, it will run everything that is (over)due. You can increase or decrease
		# the accuracy of the execution timing by specifying smaller or larger tick
		# duration. If you are scheduling a lot of tasks you should consider increasing
		# the ticks per wheel.
		# Note that it might take up to 1 tick to stop the Timer, so setting the
		# tick-duration to a high value will make shutting down the actor system
		# take longer.
		tick-duration = 1ms

		# The timer uses a circular wheel of buckets to store the timer tasks.
		# This should be set such that the majority of scheduled timeouts (for high
		# scheduling frequency) will be shorter than one rotation of the wheel
		# (ticks-per-wheel * ticks-duration)
		# THIS MUST BE A POWER OF TWO!
		ticks-per-wheel = 512

		# This setting selects the timer implementation which shall be loaded at
		# system start-up.
		# The class given here must implement the akka.actor.Scheduler interface
		# and offer a public constructor which takes three arguments:
		#  1) com.typesafe.config.Config
		#  2) akka.event.LoggingAdapter
		#  3) java.util.concurrent.ThreadFactory
		implementation = akka.actor.LightArrayRevolverScheduler

		# When shutting down the scheduler, there will typically be a thread which
		# needs to be stopped, and this timeout determines how long to wait for
		# that to happen. In case of timeout the shutdown of the actor system will
		# proceed without running possibly still enqueued tasks.
		shutdown-timeout = 5s
	}
}

kamon {

	akka.actor-groups = [ "Router","PM-Children" ]
	util.filters {
		"akka.tracked-actor" {
			includes = ["dockerexp/user/Manager_*","dockerexp/user/router","dockerexp/user/UpdateGen","dockerexp/system/**", "dockerexp/user/Manager_*_child_*","dockerexp/user/router/child_*"],
			excludes = []
		}

		"PM-Children" {
			includes = [ "dockerexp/user/Manager_*_child_*" ]
		}
		"Router" {
			includes = [ "dockerexp/user/router/child_*" ]
		}
	}

	prometheus {
		embedded-server {
			# Hostname and port used by the embedded web server to publish the scraping enpoint.
			hostname = 0.0.0.0
			port = 11600
		}
		buckets {
			default-buckets = [
				10
			]
			time-buckets = [
				1
			]
			information-buckets = [
				1024
			]
		}
	}
}
worker-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}
reader-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 1
}

archivist-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 100
}
logging-dispatcher {
	# Dispatcher is the name of the event-based dispatcher
	type = Dispatcher
	# What kind of ExecutionService to use
	executor = "fork-join-executor"
	# Configuration for the fork join pool
	fork-join-executor {
		# Min number of threads to cap factor-based parallelism number to
		parallelism-min = 2
		# Parallelism (threads) ... ceil(available processors * factor)
		parallelism-factor = 2.0
		# Max number of threads to cap factor-based parallelism number to
		parallelism-max = 10
	}
	# Throughput defines the maximum number of messages to be
	# processed per actor before the thread jumps to the next actor.
	# Set to 1 for as fair as possible.
	throughput = 100
}